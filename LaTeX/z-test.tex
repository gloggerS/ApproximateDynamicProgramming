\documentclass{scrbook}
\usepackage[utf8]{inputenc}

% Pseudo Code
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{mathtools}
\usepackage{bbm}

\usepackage[subnum]{cases}

\usepackage{cleveref}

\usepackage{todonotes}
\newcommand{\todoRed}[1]{\todo[inline, color=red!40]{#1}}
\newcommand{\todoMinor}[1]{\todo[inline, color=orange!20]{#1}}

\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\title{Approximate Dynamic Programming}
\author{Stefan Glogger}
\date{June 2019}

\begin{document}

\section{Approximate Dynamic Programming}

\subsection{Approximate Policy Iteration}

\begin{algorithm}
	\caption{Approximate policy iteration}\label{alg-API}
	\begin{algorithmic}[1]
		\State Set $\theta_t = 0$ and $\mathbf{\pi}_t = \mathbf{0}$ $\forall t = 1, \dots, T$
		\For{\texttt{k = 1 to K}}
			\State Set $\hat{V}_t = 0$ and $\mathbf{\hat{C}}_t = 0$ $\forall t = 1, \dots, T$
			\For{\texttt{i = 1 to I}}
				\State Set $\hat{r}_t = 0$ and $\mathbf{\hat{c}}_t = 0$ $\forall t = 1, \dots, T$
				\State Initialize $\mathbf{c} = \mathbf{c}^0$
				\For{\texttt{t = 1 to T}}
					\State $\mathbf{\hat{c}}_t \coloneqq \mathbf{c}$
					\State Compute $\mathbf{\pi}(t, \mathbf{c})$ \label{alg-API-calcPi}
					\State Compute $\mathbf{x} = \text{determineOfferset}(\mathbf{\pi}(t, \mathbf{c}), \epsilon_t)$
					\State Simulate a sales event $j' \in \{0, 1, \dots, n\}$
					\If{$j' \in \{ 1, \dots, n\}$}
						\State $\hat{r}_t = r_{j'}$ and $\mathbf{c} = \mathbf{c} - \mathbf{a}_{j'}$
					\EndIf
				\EndFor
				\State Compute $\hat{V}_t^i = \sum_{\tau = t}^{T}\hat{r}_t \quad \forall t = 1, \dots, T$
				\State Assign $\mathbf{\hat{C}}_t^i = \mathbf{\hat{c}}_t \quad \forall t = 1, \dots, T$
			\EndFor
			\State $\left(\theta_t, \pi_t \right) = \text{updateParameters}\left(\hat{V}_t, \mathbf{\hat{C}}_t, \theta_t, \pi_t, k\right) \quad \forall t = 1, \dots, T$ \label{alg-API-updateParam}
		\EndFor
		\Return {$\left(\theta_t, \pi_t \right)  \quad \forall t = 1, \dots, T$}
	\end{algorithmic}
\end{algorithm}

\todoMinor{\Cref{alg-API-updateParam} umschreiben. Weil fuer den update werden alle Parameter auf einmal geupdatet und nicht fuer jeden Zeitschritt separat.}

Overview of parameters:
\begin{enumerate}
	\item $\theta_t$	optimization parameter (offset)
	\item $\mathbf{\pi}_t$	optimization parameter (bid price for each resource)
	\item $\hat{V}_t = 0$	all sample revenues to go for each sample for each time
	\item $\mathbf{\hat{C}}_t = 0$	all sample available capacities for each sample for each resource for each time
	\item $r_t$	sample revenue generated at time $t$
	\item $\mathbf{c}$	available capacities for each resource at current time
	\item $\mathbf{c}^0$	starting capacities
	\item $\mathbf{x}$ offerset at current time
	\item $\epsilon_t$ epsilon used at current time
	
\end{enumerate}

Calculations:

\noindent\rule{\textwidth}{1pt}
$\mathbf{\pi}(t, \mathbf{c}) = \pi_h(t, c_h) \text{ for } h \in [m]$

\begin{numcases}{\pi_h(t, c_h) = }
\infty & if $c_h = 0$ \\
\sum_{s=1}^{S_h} \pi_{ths}\mathbbm{1}_{\left(b_h^{s-1}, b_h^s\right]}(c_h) &  otherwise.
\end{numcases}

\todoRed{Fuer \Cref{alg-API-calcPi} verwende Zeit \textbf{t} statt \textbf{t+1}. Grund: Kenne Informationen zur Zukunft nicht.}

\noindent\rule{\textwidth}{1pt}
The function $\text{determineOfferset}(\mathbf{\pi}, \epsilon)$ calculates the set to offer depending on the current bid prices $\mathbf{\pi}$ via the greedy algorithm layed out in 
%todo add reference
. To account for the exploration vs exploitation dilemma, an epsilon-greedy strategy is used. With a probability of $\epsilon/2$ either no product is offered at all or all products with positive contribution $r_j - \sum_{h \in [m]} a_{hj} \cdot \pi_h$ are offered. With a probability of $1-\epsilon$, the proper calculated set is offered.

\noindent\rule{\textwidth}{1pt}
A sales event is simulated by first having one or zero customer arrive at random. In case a customer arrives, its preference function given the offer set determines the probability according to which one product is sold ($j' \in \{1, \dots, n\}$) or no product is sold ($j' = 0$).

\noindent\rule{\textwidth}{1pt}
%todo Schreibweise der Funktion anpassen, da alle Parameter auf eimal Ã¼bergeben werden, und nicht separat je Zeitschritt
The function $\left(\theta_t, \pi_t \right) = \text{updateParameters}\left(\hat{V}_t, \mathbf{\hat{C}}_t, \theta_t, \pi_t, k\right)$ really optimizes the following least squares optimization problem for all parameters ($t = 1, \dots, T$) at the same time.

\begin{align}
	V_t(\theta_t, \mathbf{\pi}_t, \mathbf{c}_t) & \coloneqq \theta_t + \sum_{h=1}^{m}\sum_{s=1}^{S_h} \pi_{ths} f_{hs}(c_h) \\
	f_{hs}(c_h) &\coloneqq 
	\begin{cases}\label{def-f}
		0 & \text{ if } c_h \leq b_h^{s-1}\\
		c_h - b_h^{s-1} & \text{ if } b_h^{s-1} < c_h \leq b_h^s \\
		b_h^s - b_h^{s-1} & \text{ if } b_h^s < c_h
	\end{cases}
\end{align}

\Cref{def-f} describes the occupied amount of capacity of interval $\left(b_h^{s-1}, b_h^s\right]$.

The following optimization problem depends on the old parameters $\theta_t = \theta_t^k$ and $\mathbf{\pi}_t = \mathbf{\pi}_t^k$ to determine the optimal parameter $\theta_t^{update}$ and $\mathbf{\pi}_t^{update}$.

\begin{alignat}{2}
 & \text{min} \sum_{i=1}^{I}\sum_{t=1}^{T} \left( \hat{V}_t^i - V_t(\theta_t, \mathbf{\pi}_t, \mathbf{c}_t^i) \right)^2 && \\
 & s.t. && \\
 & \theta_t \geq 0 && \forall t\\
 & \max_{j=1, \dots, n} r_j \geq \pi_{ths} \geq 0 && \forall t, h, s\\
 & \pi_{ths} \geq \pi_{th,s+1} && \forall t, h, s = 1, \dots, S_h-1\\
 & \theta_t \geq \theta_{t+1} && \forall t = 1, \dots, T-1\\
 & \pi_{ths} \geq \pi_{t+1,hs} && \forall t = 1, \dots, T-1
\end{alignat}

The final parameters $\theta_t^{K+1}$ and $\mathbf{\pi}_t^{K+1}$ can obtained via two possible equally possible ways. One is the so called exponential smoothing, where in each iteration $k$ the parameter for the next iteration $k+1$ is calculated via:
\begin{align}
	\theta_t^{k+1} &= \left(1- \frac{1}{k} \right)	\theta_t^k + \frac{1}{k} \theta_t^{update}\\
	\mathbf{\pi}_t^{k+1} &= \left(1- \frac{1}{k} \right)	\mathbf{\pi}_t^k + \frac{1}{k} \mathbf{\pi}_t^{update}
\end{align}
The other one uses $\theta_t^{k+1} = \theta_t^{update}$ and $\mathbf{\pi}_t^{k+1} = \mathbf{\pi}_t^{update}$ and averages at the very end.
\begin{align}
	\theta_t^{K+1} &= \frac{1}{K}\sum_{k=1}^{K}\theta_t^k\\
	\mathbf{\pi}_t^{K+1} &= \frac{1}{K}\sum_{k=1}^{K}\mathbf{\pi}_t^k
\end{align}

\begin{proof}
	\todoMinor{Den Beweis sauber ausfuehren. Hier sind die Inhalte. Die Aussage stimmt, falls die gefundenen optimalen Loesungen in jeder Iteration stets dieselben sind. Dies ist der Fall, wenn es stets nur ein Minimum gibt. Wir haben eine quadratische Zielfunktion, die sozusagen eine mehrdimensionale, nach oben geoeffnete Parabel zeigt, die genau ein globales Minimum besitzt. Weitere noetige Punkte: eindeutiges globales Minimum ueber positiv definite Hesse-Matrix. Optimum auch zulaessig (schwierig?)}
\end{proof}

\end{document}
